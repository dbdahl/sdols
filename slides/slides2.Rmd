---
title: |
  | Summarizing Distributions of
  | Latent Structures
author: |
  | David B. Dahl: Brigham Young University
  | Peter Müller: University of Texas at Austin
  |
  | Bayesian Nonparametric Inference: Dependence Structures & Applications
  | Oaxaca, Mexico
  | December 6, 2017
#date: "December 6, 2017"

fontsize: 11pt
output:
  beamer_presentation:
    theme: "metropolis"
    #colortheme: "dolphin"
    fonttheme: "professionalfonts"
    includes:
      in_header: latex-topmatter.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
knitr::opts_chunk$set(cache=TRUE)
options(width = 50)

library(sdols)
iris.clusterings <- unname(iris.clusterings)
tmp <- iris.clusterings[1,]
iris.clusterings[1,] <- iris.clusterings[13,]
iris.clusterings[13,] <- tmp
```

## Motivation

- In a typical Bayesian analysis, consider effort is placed
on "**fitting the model**" (e.g., sampling from the posterior)
but this is **only half of the inference problem**.

- Meaningful inference also requires **summarizing the posterior distribution** of
the parameters of interest.

- Posterior summaries are important for subsequent analyses or in
communicating the results to a diverse audience.

- If the parameters of interest live in $\mathbb{R}^n$, common
posterior summaries are **means** and **medians**.

- Summarizing posterior distributions of parameters with **complicated structure**
is more challenging, e.g., the "average" network in the network distribution
is not easily defined.

- We consider summarizing distributions of latent structures, e.g.,
clusterings, feature allocations, networks, and variable selection.

## Overview

- We present the **sequentially-allocated latent structure optimization (SALSO)** method to
minimize an objective criterion to obtain a *point estimate* from a randomly-sampled collection
of *latent features*.

- SALSO is a *global optimization* method involving a *series of micro optimizations*.

- The method can be applied to *clusterings*, *feature allocations*, *networks*, *variable selection*, etc.

- Several objective criterion can be used, including squared error loss, absolute error loss, Binder (1978)
loss, or the lower bound of the variation of information loss (Wade & Ghahramani 2017), respectively.

## Example: Dirichlet Mixture Model with Gaussian Likelihood

Suppose MCMC output contains 1,000 posterior samples for the partition of 150 observations (encoded with cluster labels), e.g.:
```{r, echo=TRUE}
iris.clusterings[1,]
```
What is the Bayes estimate of the partition from these 1,000 samples?

## Example: First Clustering in MCMC Output

Clustering in set **_partition_** notation:
$$\pi^{(1)} = \{ \{ 1, 3\}, \{2, 4, 5\} \}$$

. . .

Clustering in **_cluster label_** notation:
$$c^{(1)} = (1,2,1,2,2)$$

. . .

Clustering as **_pairwise allocation matrix_** (i.e., adjacency matrix):
$$
A(c^{(1)}) = 
\begin{bmatrix}
1 & 0 & 1 & 0 & 0\\
0 & 1 & 0 & 1 & 1\\
1 & 0 & 1 & 0 & 0\\
0 & 1 & 0 & 1 & 1\\
0 & 1 & 0 & 1 & 1\\
\end{bmatrix}
$$

## Example: Second Clustering in MCMC Output

Clustering in set **_partition_** notation:
$$\pi^{(2)} = \{ \{ 1, 2, 3\}, \{4\}, \{5\} \}$$

Clustering in **_cluster label_** notation:
$$c^{(2)} = (1,1,1,2,3)$$

Clustering as **_pairwise allocation matrix_** (i.e., adjacency matrix):
$$
A(c^{(2)}) = 
\begin{bmatrix}
1 & 1 & 1 & 0 & 0\\
1 & 1 & 1 & 0 & 0\\
1 & 1 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}
$$

## Example: Third Clustering in MCMC Output

Clustering in set **_partition_** notation:
$$\pi^{(3)} = \{ \{1, 2, 4\}, \{3, 5\} \}$$

Clustering in **_cluster label_** notation:
$$c^{(3)} = (1,1,2,1,2)$$

Clustering as **_pairwise allocation matrix_** (i.e., adjacency matrix):
$$
A(c^{(3)}) = 
\begin{bmatrix}
1 & 1 & 0 & 1 & 0\\
1 & 1 & 0 & 1 & 0\\
0 & 0 & 1 & 0 & 1\\
1 & 1 & 0 & 1 & 0\\
0 & 0 & 1 & 0 & 1\\
\end{bmatrix}
$$

## Example: Averaging the MCMC Clustering Output
 
Averaging the set partitions is **not defined**.
$$( \{ \{ 1, 3\}, \{2, 4, 5\} \} \ + \ \{ \{ 1, 2, 3\}, \{4\}, \{5\} \} \ + \  \{ \{ 1, 2, 4\}, \{3, 5\} \} ) / 3 \ = \ ???$$

. . .

Averaging the vector of cluster labels **does not make sense**.
$$( \ (1,2,1,2,2) \ + \ (1,1,1,2,3) \ + \ (1,1,2,1,2) \ ) \ / \ 3 = (1,\ 4/3,\  4/3,\ 5/3,\ 2)$$

. . .

Averaging allocation matrices **does** make sense:
$$
\bar{A} = \frac{1}{B} \sum_{b=1}^B A(c^{(b)}) = 
\begin{bmatrix}
1 & 2/3 & 2/3 & 1/3 & 0\\
2/3 & 1 & 1/3 & 2/3 & 1/3\\
2/3 & 1/3 & 1 & 0 & 1/3\\
1/3 & 2/3 & 0 & 1 & 1/3\\
0 & 1/3 & 1/3 & 1/3 & 1\\
\end{bmatrix}
$$
We call this the **estimated expected allocation matrix (EPAM)** because it's $(i,j)$ element
estimates $\mu_{ij} = \text{Pr}( c_i = c_j \mid \text{data})$.

## Loss Functions and Bayes Estimators

- A Bayes estimator minimizes the posterior expected value of a loss function.

- The 0-1 loss function:
  $$L(c,\hat{c}) = \I{c = \hat{c}}$$
  yielding the maximum _a posteriori_ (MAP) clustering:
  $$\text{argmax}_{\hat{c}} \ p(\hat{c} \given \text{data})$$.
    - Equal costs for a clustering that differs by one label and a clustering that differs by many labels.
    - Mode may not represent well the ``center'' of a distribution.
        
## Loss Functions and Bayes Estimators

- Dahl (2006) suggested the least-squares clustering:
    $$\text{argmin}_{\hat{c}} \ \sum_{i=1}^n \sum_{j=1}^n ( A(\hat{c})_{ij} - \mu_{ij} )^2$$
- Lau & Green (2007) studied the Binder (1978) loss function in a Bayesian nonparametric context:
    $$L(c,\hat{c}) = \sum_{i<j} \I{c_i     = c_j} \ \I{\hat{c}_i \not= \hat{c}_j} +
                                \I{c_i \not= c_j} \ \I{\hat{c}_i     = \hat{c}_j}$$
    yielding the clustering:
    $$\text{argmin}_{\hat{c}} \ \sum_{i=1}^n \sum_{j=1}^n \I{\hat{c}_i=\hat{c}_j} ( 0.5 - \mu_{ij} )$$
- Dahl & Newton (2007) noted that minimizing the posterior expected loss of Binder (1978) is
equilvalent to the least-squares criterion in Dahl (2006).
    
## Loss Functions and Bayes Estimators

- Wade & Ghahramani (2017) proposed using the variation of information (VI) of Meilă (2007) as
    a loss function, yielding the clustering:
    $$\begin{split}\text{argmin}_{\hat{c}} \sum_{i=1}^n \bigg( & \log\bigg(\sum_{j=1}^n \I{\hat{c}_j = \hat{c}_i}\bigg) \\
      & - 2 \text{E} \bigg( \log \bigg( \sum_{j=1}^n \I{\hat{c}_j = \hat{c}_i, c_j = c_i} \bigg) \bigg| \text{data} \bigg) \bigg)\end{split}$$
    which is computationally expensive to compute.  Instead, they suggest the clustering that
    minimizes the **lower bound** of the expected value of the posterior expected loss of variation
    of information (VI.lb):
    $$\text{argmin}_{\hat{c}} \sum_{i=1}^n \bigg( \log\bigg(\sum_{j=1}^n \I{\hat{c}_j = \hat{c}_i}\bigg)
      - 2 \log \bigg( \sum_{j=1}^n \I{\hat{c}_j = \hat{c}_i} \mu_{ij} \bigg) \bigg)$$

## Monte Carlo Estimate the Posterior Expected Loss

- For a given $\hat{c}$, both the Binder and the lower bound of the VI loss are based on the $\mu_{ij}$'s.
- The $(i,j)$ elements of $\bar{A}$ are Monte Carlo estimates of the $\mu_{ij}$'s, leading to 
a Monte Carlo estimate of the posterior expected loss.
- But having a means to estimate the posterior expected loss for a given $\hat{c}$ does not itself give
a search algorithm for its minimization.

## Methods for Optimization Given a Loss Function

- Exhaustive search. Infeasible for even moderate $n$, e.g., $B(15) = 1,382,958,545$.
- Rounding the estimated expected pairwise allocation matrix (EPAM).  May not lead to a clustering, e.g.:
$$
\bar{A} = 
\begin{bmatrix}
1 & 2/3 & 2/3 & 1/3 & 0\\
2/3 & 1 & 1/3 & 2/3 & 1/3\\
2/3 & 1/3 & 1 & 0 & 1/3\\
1/3 & 2/3 & 0 & 1 & 1/3\\
0 & 1/3 & 1/3 & 1/3 & 1\\
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
1 & 1 & 1 & 0 & 0\\
1 & 1 & 0 & 1 & 0\\
1 & 0 & 1 & 0 & 0\\
0 & 1 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}
$$
- Medvedovic and Sivaganesan (2002) selected a clustering using hierarchal
clustering using $1-\bar{A}$ as the distance matrix.
- Dahl (2006) selected the clustering in the MCMC output that minimizes the criterion.

## Methods for Optimization Given a Loss Function

- Lau & Green (2007) proposed a heuristic item-swapping algorithm based on binary integer programming to
minimize the posterior expected Binder loss.
- Wade & Ghahramani (2017) proposed a greedy search algorithm based on neighborhood search defined by
the Hasse diagram, which can be used for Binder or VI.lb loss.
- We propose the **sequentially-allocated latent structure optimization (SALSO)** method to perform
a *series of micro optimizations* to obtain a global minimum of posterior expected value of Binder or VI.lb loss.

## Sequentially-Allocated Latent Structure Optimization

- The SALSO method is applicable for many types of latent structure, including clusterings, feature allocations, & networks.
- The steps to SALSO are:
    - Build up a full structure by sequentially optimizing the allocation of items.
    - Improve the full structure by a series of univariate label optimzations.
    - Do the above steps many times for random permutations and select the structure that minimizes
      the expected posterior loss.
- The random order in which items are allocated is not necessarily their order in the
dataset; the permutation $\Permutation = (\permutation_1,\ldots,\permutation_n)$
of $\{1,\ldots,n\}$ gives the sequence
in which the $n$ items are allocated, where the $t^{\text{th}}$ item allocated
is $\permutation_t$.
- For clarity, let's examing SALSO in the clustering context.

## Step 1: Build Up a Full Structure


- Let $c_{\permutation_t}$ be the clustering of items $(\permutation_1,\ldots,\permutation_t)$.

This is an R Markdown presentation. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

## Slide with Bullets

- Bullet 1
- Bullet 2
- Bullet 3

## Slide with R Output

This is very important.
```{r conf, echo=TRUE}
library(sdols)
ppm <- expectedPairwiseAllocationMatrix(iris.clusterings)
est <- salso(ppm)
conf <- confidence(est,ppm)
#plot(conf)
plot(conf,data=iris)
```

## Another slide

It is very important $\sum_i^n x^2$.

$$\sum_{i=1}^n (x_i - \bar{x})^2$$

```{r cars, echo = TRUE}
summary(cars)
```

## Slide with Plot

```{r pressure}
plot(pressure)
```

