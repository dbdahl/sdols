---
title: |
  | Summarizing Distributions of
  | Latent Structures
author: |
  | David B. Dahl: Brigham Young University
  | Peter MÃ¼ller: University of Texas at Austin
  |
  | Bayesian Nonparametric Inference: Dependence Structures & Applications
  | Oaxaca, Mexico
  | December 6, 2017
#date: "December 6, 2017"

fontsize: 11pt
output:
  beamer_presentation:
    theme: "metropolis"
    #colortheme: "dolphin"
    fonttheme: "structurebold"
    includes:
      in_header: latex-topmatter.tex
---

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
knitr::opts_chunk$set(cache=TRUE)
options(width = 50)

library(sdols)
iris.clusterings <- unname(iris.clusterings)
tmp <- iris.clusterings[1,]
iris.clusterings[1,] <- iris.clusterings[13,]
iris.clusterings[13,] <- tmp
```

## Motivation

- In a typical Bayesian analysis, consider effort is placed
on "**fitting the model**" (e.g., sampling from the posterior)
but this is **only half of the inference problem**.

- Meaningful inference also requires **summarizing the posterior distribution** of
the parameters of interest.

- Posterior summaries are important for subsequent analyses or in
communicating the results to a diverse audience.

- If the parameters of interest live in $\mathbb{R}^n$, common
posterior summaries are **means** and **medians**.

- Summarizing posterior distributions of parameters with **complicated structure**
is more challenging, e.g., the "average" network in the network distribution
is not easily defined.

- We consider summarizing distributions of latent structures, e.g.,
clusterings, feature allocations, networks, and variable selection.

## Overview

- We present the **sequentially-allocated latent structure optimizations (SALSO)** method to
minimize an objective criterion to obtain a *point estimate* from a randomly-sampled collection
of *latent features*.

- SALSO is a *global optimization* method involving a *series of micro optimizations*.

- The method can be applied to *clusterings*, *feature allocations*, *networks*, *variable selection*, etc.

- Several objective criterion can be used, including squared error loss, absolute error loss, Binder (1978)
loss, or the lower bound of the variation of information loss (Wade & Ghahramani 2017), respectively.

## Example: Dirichlet Mixture Model with Gaussian Likelihood

Suppose MCMC output contains 1,000 posterior samples for the partition of 150 observations (encoded with cluster labels), e.g.:
```{r, echo=TRUE}
iris.clusterings[1,]
```
What is the Bayes estimate of the partition from these 1,000 samples?

## Example: First Clustering in MCMC Output

Clustering in set **_partition_** notation:
$$\pi_1 = \{ \{ 1, 3\}, \{2, 4, 5\} \}$$

. . .

Clustering in **_cluster label_** notation:
$$c_1 = (1,2,1,2,2)$$

. . .

Clustering as **_pairwise allocation matrix_** (i.e., adjacency matrix):
$$
A_1 = 
\begin{bmatrix}
1 & 0 & 1 & 0 & 0\\
0 & 1 & 0 & 1 & 1\\
1 & 0 & 1 & 0 & 0\\
0 & 1 & 0 & 1 & 1\\
0 & 1 & 0 & 1 & 1\\
\end{bmatrix}
$$

## Example: Second Clustering in MCMC Output

Clustering in set **_partition_** notation:
$$\pi_2 = \{ \{ 1, 2, 3\}, \{4\}, \{5\} \}$$

Clustering in **_cluster label_** notation:
$$c_2 = (1,1,1,2,3)$$

Clustering as **_pairwise allocation matrix_** (i.e., adjacency matrix):
$$
A_2 = 
\begin{bmatrix}
1 & 1 & 1 & 0 & 0\\
1 & 1 & 1 & 0 & 0\\
1 & 1 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}
$$

## Example: Third Clustering in MCMC Output

Clustering in set **_partition_** notation:
$$\pi_3 = \{ \{1, 2, 4\}, \{3, 5\} \}$$

Clustering in **_cluster label_** notation:
$$c_3 = (1,1,2,1,2)$$

Clustering as **_pairwise allocation matrix_** (i.e., adjacency matrix):
$$
A_3 = 
\begin{bmatrix}
1 & 1 & 0 & 1 & 0\\
1 & 1 & 0 & 1 & 0\\
0 & 0 & 1 & 0 & 1\\
1 & 1 & 0 & 1 & 0\\
0 & 0 & 1 & 0 & 1\\
\end{bmatrix}
$$

## Example: Averaging the MCMC Clustering Output
 
Averaging the set partitions is **not defined**.
$$( \{ \{ 1, 3\}, \{2, 4, 5\} \} \ + \ \{ \{ 1, 2, 3\}, \{4\}, \{5\} \} \ + \  \{ \{ 1, 2, 4\}, \{3, 5\} \} ) / 3 \ = \ ???$$

. . .

Averaging the vector of cluster labels **does not make sense**.
$$( \ (1,2,1,2,2) + (1,1,1,2,3) + (1,1,2,1,2) \ ) / 3 = (1,\ 4/3,\  4/3,\ 5/3,\ 2) $$

. . .

Averaging allocation matrices **does** make sense:
$$
\frac{1}{B} \sum_{b=1}^B A_b = 
\begin{bmatrix}
1 & 2/3 & 2/3 & 1/3 & 0\\
2/3 & 1 & 1/3 & 2/3 & 2/3\\
2/3 & 1/3 & 1 & 0 & 1/3\\
1/3 & 2/3 & 0 & 1 & 1/3\\
0 & 1/3 & 1/3 & 1/3 & 1\\
\end{bmatrix}
$$
We call this the **estimated expected allocation matrix (EPAM)**.

## Methods for Clustering Estimation Based on the EPAM

Decision theory approach:

$$

$$

## R Markdown

This is an R Markdown presentation. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

## Slide with Bullets

- Bullet 1
- Bullet 2
- Bullet 3

## Slide with R Output

This is very important.
```{r conf, echo=TRUE}
library(sdols)
ppm <- expectedPairwiseAllocationMatrix(iris.clusterings)
est <- salso(ppm)
conf <- confidence(est,ppm)
#plot(conf)
plot(conf,data=iris)
```

## Another slide

It is very important $\sum_i^n x^2$.

$$\sum_{i=1}^n (x_i - \bar{x})^2$$

```{r cars, echo = TRUE}
summary(cars)
```

## Slide with Plot

```{r pressure}
plot(pressure)
```

