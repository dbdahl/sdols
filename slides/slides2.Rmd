---
title: |
  | Summarizing Distributions of
  | Latent Structures
author: |
  | David B. Dahl: Brigham Young University
  | Peter Müller: University of Texas at Austin
  |
  | Bayesian Nonparametric Inference: Dependence Structures & Applications
  | Oaxaca, Mexico
  | December 6, 2017
#date: "December 6, 2017"

fontsize: 11pt
output:
  beamer_presentation:
    theme: "metropolis"
    #colortheme: "dolphin"
    fonttheme: "professionalfonts"
    includes:
      in_header: latex-topmatter.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
knitr::opts_chunk$set(cache=TRUE)
options(width = 50)

library(sdols)
iris.clusterings <- unname(iris.clusterings)
tmp <- iris.clusterings[1,]
iris.clusterings[1,] <- iris.clusterings[13,]
iris.clusterings[13,] <- tmp
```

## Motivation

- In a typical Bayesian analysis, consider effort is placed
on "**fitting the model**" (e.g., sampling from the posterior)
but this is **only half of the inference problem**.

- Meaningful inference also requires **summarizing the posterior distribution** of
the parameters of interest.

- Posterior summaries are important for subsequent analyses or in
communicating the results to a diverse audience.

- If the parameters of interest live in $\mathbb{R}^n$, common
posterior summaries are **means** and **medians**.

- Summarizing posterior distributions of parameters with **complicated structure**
is more challenging, e.g., the "average" network in the network distribution
is not easily defined.

- We consider summarizing distributions of latent structures, e.g.,
clusterings, feature allocations, networks, and variable selection.

## Overview

- We present the **sequentially-allocated latent structure optimizations (SALSO)** method to
minimize an objective criterion to obtain a *point estimate* from a randomly-sampled collection
of *latent features*.

- SALSO is a *global optimization* method involving a *series of micro optimizations*.

- The method can be applied to *clusterings*, *feature allocations*, *networks*, *variable selection*, etc.

- Several objective criterion can be used, including squared error loss, absolute error loss, Binder (1978)
loss, or the lower bound of the variation of information loss (Wade & Ghahramani 2017), respectively.

## Example: Dirichlet Mixture Model with Gaussian Likelihood

Suppose MCMC output contains 1,000 posterior samples for the partition of 150 observations (encoded with cluster labels), e.g.:
```{r, echo=TRUE}
iris.clusterings[1,]
```
What is the Bayes estimate of the partition from these 1,000 samples?

## Example: First Clustering in MCMC Output

Clustering in set **_partition_** notation:
$$\pi^{(1)} = \{ \{ 1, 3\}, \{2, 4, 5\} \}$$

. . .

Clustering in **_cluster label_** notation:
$$c^{(1)} = (1,2,1,2,2)$$

. . .

Clustering as **_pairwise allocation matrix_** (i.e., adjacency matrix):
$$
A(c^{(1)}) = 
\begin{bmatrix}
1 & 0 & 1 & 0 & 0\\
0 & 1 & 0 & 1 & 1\\
1 & 0 & 1 & 0 & 0\\
0 & 1 & 0 & 1 & 1\\
0 & 1 & 0 & 1 & 1\\
\end{bmatrix}
$$

## Example: Second Clustering in MCMC Output

Clustering in set **_partition_** notation:
$$\pi^{(2)} = \{ \{ 1, 2, 3\}, \{4\}, \{5\} \}$$

Clustering in **_cluster label_** notation:
$$c^{(2)} = (1,1,1,2,3)$$

Clustering as **_pairwise allocation matrix_** (i.e., adjacency matrix):
$$
A(c^{(2)}) = 
\begin{bmatrix}
1 & 1 & 1 & 0 & 0\\
1 & 1 & 1 & 0 & 0\\
1 & 1 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}
$$

## Example: Third Clustering in MCMC Output

Clustering in set **_partition_** notation:
$$\pi^{(3)} = \{ \{1, 2, 4\}, \{3, 5\} \}$$

Clustering in **_cluster label_** notation:
$$c^{(3)} = (1,1,2,1,2)$$

Clustering as **_pairwise allocation matrix_** (i.e., adjacency matrix):
$$
A(c^{(3)}) = 
\begin{bmatrix}
1 & 1 & 0 & 1 & 0\\
1 & 1 & 0 & 1 & 0\\
0 & 0 & 1 & 0 & 1\\
1 & 1 & 0 & 1 & 0\\
0 & 0 & 1 & 0 & 1\\
\end{bmatrix}
$$

## Example: Averaging the MCMC Clustering Output
 
Averaging the set partitions is **not defined**.
$$( \{ \{ 1, 3\}, \{2, 4, 5\} \} \ + \ \{ \{ 1, 2, 3\}, \{4\}, \{5\} \} \ + \  \{ \{ 1, 2, 4\}, \{3, 5\} \} ) / 3 \ = \ ???$$

. . .

Averaging the vector of cluster labels **does not make sense**.
$$( \ (1,2,1,2,2) \ + \ (1,1,1,2,3) \ + \ (1,1,2,1,2) \ ) \ / \ 3 = (1,\ 4/3,\  4/3,\ 5/3,\ 2)$$

. . .

Averaging allocation matrices **does** make sense:
$$
\bar{A} = \frac{1}{B} \sum_{b=1}^B A(c^{(b)}) = 
\begin{bmatrix}
1 & 2/3 & 2/3 & 1/3 & 0\\
2/3 & 1 & 1/3 & 2/3 & 2/3\\
2/3 & 1/3 & 1 & 0 & 1/3\\
1/3 & 2/3 & 0 & 1 & 1/3\\
0 & 1/3 & 1/3 & 1/3 & 1\\
\end{bmatrix}
$$
We call this the **estimated expected allocation matrix (EPAM)** because it's $(i,j)$ element
estimates $\mu_{ij} = \text{Pr}( c_i = c_j \mid \text{data})$.

## Approaches to Clustering Estimation

- A Bayes estimator minimizes the posterior expected value of a loss function.

- The 0-1 loss function:
  $$L(c,\hat{c}) = \I{c = \hat{c}}$$
  yielding the maximum _a posteriori_ (MAP) clustering:
  $$\hat{c} = \text{argmax}_c \ p(c \given \text{data})$$.
    - Equal costs for a clustering that differs by one label and a clustering that differs by many labels.
    - Mode may not represent well the ``center'' of a distribution.
        
## Approaches to Clustering Estimation

- Dahl (2006) suggested the least-squares clustering:
    $$\hat{c} = \text{argmin}_c \ \sum_{i=1}^n \sum_{j=1}^n ( A(c)_{ij} - \mu_{ij} )^2$$
- Lau & Green (2007) studied the Binder (1978) loss function in a Bayesian nonparametric context:
    $$L(c,\hat{c}) = \sum_{i<j} \I{c_i     = c_j} \ \I{\hat{c}_i \not= \hat{c}_j} +
                                \I{c_i \not= c_j} \ \I{\hat{c}_i     = \hat{c}_j}$$
    yielding the clustering:
    $$\hat{c} = \text{argmin}_c \ \sum_{i=1}^n \sum_{j=1}^n \I{\hat{c}_i=\hat{c}_j} ( 0.5 - \mu_{ij} )$$
- Dahl & Newton (2007) noted that minimizing the posterior expected loss of Binder (1978) is
equilvalent to the least-squares criterion in Dahl (2006).
    
## Approaches to Clustering Estimation

- Wade & Ghahramani (2017) studied the variation of information (VI) of Meilă (2007).

    yielding the clustering:
    $$\text{argmin}_c \sum_{i=1}^n \log\bigg(\sum_{j=1}^n \I{\hat{c}_j = \hat{c}_i}\bigg)
    $$

Decision theory approach:

$$

$$

## R Markdown

This is an R Markdown presentation. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

## Slide with Bullets

- Bullet 1
- Bullet 2
- Bullet 3

## Slide with R Output

This is very important.
```{r conf, echo=TRUE}
library(sdols)
ppm <- expectedPairwiseAllocationMatrix(iris.clusterings)
est <- salso(ppm)
conf <- confidence(est,ppm)
#plot(conf)
plot(conf,data=iris)
```

## Another slide

It is very important $\sum_i^n x^2$.

$$\sum_{i=1}^n (x_i - \bar{x})^2$$

```{r cars, echo = TRUE}
summary(cars)
```

## Slide with Plot

```{r pressure}
plot(pressure)
```

